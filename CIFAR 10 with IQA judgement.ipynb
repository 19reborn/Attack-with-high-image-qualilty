{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_ssim\n",
    "# https://github.com/Po-Hsun-Su/pytorch-ssim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 选择 dataset\n",
    "batch_size = 10\n",
    "eps = 0.031\n",
    "step_size = 2\n",
    "iteration = 10\n",
    "K = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_state_dict(state_dict):\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    if 'state_dict' in state_dict.keys():\n",
    "        state_dict = state_dict['state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if 'sub_block' in k:\n",
    "            continue\n",
    "        if 'module' in k:\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None, :, None, None]) / self.std.type_as(x)[None, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Normalize()\n",
       "  (1): PreActResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): PreActBlock(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (shortcut): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1): PreActBlock(\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "test_dataset = torchvision.datasets.CIFAR10(\"data_CIFAR10\",train=False,download=True,transform = transform)\n",
    "test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "from model.PreActResNet18 import PreActResNet18\n",
    "net = PreActResNet18(num_classes=10)\n",
    "ckpt = filter_state_dict(torch.load(\"model/AT-AWP_cifar10_l2_preactresnet18.pth\", map_location=device))\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2471, 0.2435, 0.2616)    \n",
    "net.load_state_dict(ckpt)\n",
    "model = nn.Sequential(Normalize(mean=mean, std=std), net)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_imshow(imglist, labellist):\n",
    "    figNum = len(imglist)\n",
    "    plt.figure(figsize=(5*figNum, 5))\n",
    "    namelist = []\n",
    "    for i in range(1, figNum + 1):\n",
    "        namelist.append('img' + str(i))\n",
    "        namelist[i - 1] = plt.subplot2grid((1, figNum),(0, i - 1))\n",
    "        plt.imshow(np.transpose(imglist[i - 1].cpu(),(1,2,0)))\n",
    "        namelist[i - 1].set_title(labellist[i - 1])\n",
    "    plt.show()\n",
    "    # sp = savepath + \"img\" + str(figNum) + '.png'\n",
    "    # plt.savefig(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 Loss\n",
    "def l2(img1,img2):\n",
    "    channel = img1.size()[0]\n",
    "    height = img1.size()[1]\n",
    "    width = img1.size()[2]\n",
    "    total_dif = 0.0\n",
    "    for i in range(0,channel):\n",
    "        for j in range(0,height):\n",
    "            for k in range(0, width):\n",
    "                total_dif += (img1[i][j][k].item()-img2[i][j][k].item()) ** 2\n",
    "    return total_dif ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 Loss\n",
    "def l1(img1,img2):\n",
    "    channel = img1.size()[0]\n",
    "    height = img1.size()[1]\n",
    "    width = img1.size()[2]\n",
    "    total_dif = 0\n",
    "    for i in range(0, channel):\n",
    "        for j in range(0, height):\n",
    "            for k in range(0, width):\n",
    "                total_dif += abs(img1[i][j][k].item() - img2[i][j][k].item())\n",
    "    return total_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l0 Loss 计算差距<=0.01像素点的数目\n",
    "def l0(img1,img2):\n",
    "    channel = img1.size()[0]\n",
    "    height = img1.size()[1]\n",
    "    width = img1.size()[2]\n",
    "    total_dif = 0\n",
    "    for i in range(0, channel):\n",
    "        for j in range(0, height):\n",
    "            for k in range(0, width):\n",
    "                total_dif += 0 if abs(img1[i][j][k].item() - img2[i][j][k].item()) <= 0.01 else 1\n",
    "    return total_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, delta, y, images, lam1 , lam2 ):\n",
    "        mask = torch.ones_like(x).scatter_(1, y.unsqueeze(1), 0.)\n",
    "        rest = x[mask.bool()].view(x.size(0), x.size(1) - 1)\n",
    "        xx = torch.tensor(range(x.size(0)))\n",
    "        f = torch.nn.ReLU()\n",
    "        loss_attack = 1*f(x[xx, y] - torch.max(rest, 1)[0] + K)\n",
    "        #print(loss_attack)\n",
    "        loss_ssim = 0.01*lam1 *(1 - (pytorch_ssim.ssim(images,images+delta)))\n",
    "        #loss_L_inf = 0.1*lam2 * (torch.max(torch.abs(delta)))\n",
    "        loss_L_2 = 0.0001*lam2 * torch.dist(images,images+delta ,p=2)\n",
    "        img = images+delta\n",
    "        loss_var = 500*(torch.norm(img[:,:,1:,:] - img[:,:,:-1,:])+torch.norm(img[:,:,:,1:] - img[:,:,:,:-1]))/(images.size(0)*images.size(1)*images.size(2)*images.size(3))\n",
    "        #print(loss_var)\n",
    "        #loss_var = 100* (torch.norm(torch.Tensor(np.gradient((images+delta).detach().cpu().numpy(),axis=(2,3))))/(images.size(0)*images.size(1)*images.size(2)*images.size(3))).to(device)\n",
    "        return torch.mean(loss_attack + loss_ssim + loss_L_2)+loss_var\n",
    "        #return torch.mean(f(x[xx, y] - torch.max(rest, 1)[0] + K) + lam *(eps + torch.max(delta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_attack_MyLoss(net, images, labels, eps, step_size):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    delta = torch.zeros(images.size()).to(device)  #变量求导\n",
    "    lam1 = torch.zeros(1).to(device)\n",
    "    lam2 = torch.zeros(1).to(device)\n",
    "    #delta.uniform_(-eps, eps).to(device)\n",
    "    delta = Variable(delta, requires_grad=True)\n",
    "    lam1 = Variable(lam1,requires_grad=True)\n",
    "    lam2 = Variable(lam2,requires_grad=True)\n",
    "    # print(images.size())\n",
    "    # print(labels.size())\n",
    "    # ori_images = images.data\n",
    "    criterion = My_loss()\n",
    "    for i in range(iteration):\n",
    "        # delta.requires_grad = True\n",
    "        # lam.requires_grad = True\n",
    "        # print(images.size())\n",
    "        outputs = net(images + delta)\n",
    "        net.zero_grad()\n",
    "        loss = criterion(outputs, delta, labels,images,lam1,lam2).to(device)\n",
    "        loss.backward()\n",
    "\n",
    "        # print(delta.grad)\n",
    "        new_delta = delta - step_size * delta.grad.detach()\n",
    "        adv_images = torch.clamp(images + new_delta, min=0, max=1)\n",
    "        delta = adv_images - images\n",
    "        new_lam1 = lam1 + step_size * lam1.grad.detach()\n",
    "        if new_lam1 < 0:\n",
    "            lam1 = torch.zeros(1).to(device)\n",
    "        else:\n",
    "            lam1 = new_lam1\n",
    "            \n",
    "        new_lam2 = lam2 + step_size *0.3* lam2.grad.detach()\n",
    "        if new_lam2 < 0:\n",
    "            lam2 = torch.zeros(1).to(device)\n",
    "        else:\n",
    "            lam2 = new_lam2\n",
    "            \n",
    "        # delta = torch.clamp(delta, min=-eps, max=eps)\n",
    "        delta = Variable(delta, requires_grad=True)\n",
    "        lam1 = Variable(lam1,requires_grad=True)\n",
    "        lam2 = Variable(lam2,requires_grad=True)\n",
    "        # for i in range(0, images.size()[0]):\n",
    "        #     print(torch.max(delta[i]).item())\n",
    "        #     print(torch.min(delta[i]).item())\n",
    "    deltas = []\n",
    "    for i in range(0,images.size()[0]):\n",
    "        deltas.append(torch.max(torch.abs(delta[i])).item())\n",
    "    return images + delta, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_attack_Conventional(model, image, label, eps, step_size, iters=10):\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    loss = nn.NLLLoss()\n",
    "    ori_image = image.data\n",
    "\n",
    "    for i in range(iters):\n",
    "        image.requires_grad = True\n",
    "        output = model(image)\n",
    "        model.zero_grad()\n",
    "        cost = loss(output, label).to(device)\n",
    "        cost.backward()\n",
    "        adv_image = image + step_size * image.grad.sign()\n",
    "        delta = torch.clamp(adv_image - ori_image, min=-eps, max=eps)\n",
    "        image = torch.clamp(ori_image + delta, min=0, max=1).detach_()\n",
    "    deltas = []\n",
    "    delta = image-ori_image \n",
    "    for i in range(0,images.size()[0]):\n",
    "        deltas.append(torch.max(torch.abs(delta[i])).item())\n",
    "    return image ,deltas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsgm_attack(model, image, label, eps):\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    loss = nn.NLLLoss()\n",
    "\n",
    "    image.requires_grad = True\n",
    "    output = model(image)\n",
    "    model.zero_grad()\n",
    "    cost = loss(output, label).to(device)\n",
    "    cost.backward()\n",
    "\n",
    "    image = image + eps * image.grad.sign()\n",
    "    image = torch.clamp(image, min=0, max=1)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_l2_attack(model, images, labels, targeted=False, c=1e-4, kappa=0, max_iter=1000, learning_rate=0.01) :\n",
    "    import torch.optim as optim\n",
    "    images = images.to(device)     \n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Define f-function\n",
    "    def f(x) :\n",
    "\n",
    "        outputs = model(x)\n",
    "        one_hot_labels = torch.eye(len(outputs[0]))[labels].to(device)\n",
    "\n",
    "        i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "        j = torch.masked_select(outputs, one_hot_labels.byte())\n",
    "        \n",
    "        # If targeted, optimize for making the other class most likely \n",
    "        if targeted :\n",
    "            return torch.clamp(i-j, min=-kappa)\n",
    "        \n",
    "        # If untargeted, optimize for making the other class most likely \n",
    "        else :\n",
    "            return torch.clamp(j-i, min=-kappa)\n",
    "    \n",
    "    w = torch.zeros_like(images, requires_grad=True).to(device)\n",
    "\n",
    "    optimizer = optim.Adam([w], lr=learning_rate)\n",
    "\n",
    "    prev = 1e10\n",
    "    \n",
    "    for step in range(max_iter) :\n",
    "\n",
    "        a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "        #loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "        loss1 = torch.mean(torch.dist(images,a ,p=2))\n",
    "        #loss1 = torch.mean(torch.max(torch.abs(a-images)))\n",
    "        loss2 = torch.mean(c*f(a))\n",
    "\n",
    "        cost = loss1 + loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Early Stop when loss does not converge.\n",
    "        if step % (max_iter//10) == 0 :\n",
    "            if cost > prev :\n",
    "                print('Attack Stopped due to CONVERGENCE....')\n",
    "                delta = a -images\n",
    "                deltas=[]\n",
    "                for i in range(0,images.size()[0]):\n",
    "                    deltas.append(torch.max(torch.abs(delta[i])).item())                \n",
    "                return a, deltas\n",
    "            prev = cost\n",
    "        \n",
    "        print('- Learning Progress : %2.2f %%        ' %((step+1)/max_iter*100), end='\\r')\n",
    "\n",
    "    attack_images = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "    delta = attack_images -images\n",
    "    deltas=[]\n",
    "    for i in range(0,images.size()[0]):\n",
    "        deltas.append(torch.max(torch.abs(delta[i])).item())\n",
    "        \n",
    "    return attack_images, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-0a215467f9d9>:13: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629411241/work/aten/src/ATen/native/IndexingUtils.h:20.)\n",
      "  j = torch.masked_select(outputs, one_hot_labels.byte())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Stopped due to CONVERGENCE....\n",
      "Total Image Count: 10 Success Rate:\n",
      " SSIM_attack: 1.0 PGD_attack: 0.7 FSGM_attack: 0.2 CW_attack: 0.4\n",
      "Average SSIM: SSIM_attack: 0.979 PGD_attack: 0.957 FSGM_attack: 0.953 CW_attack: 0.988\n",
      "Average MOS: SSIM_attack: 24.095 PGD_attack: 26.238 FSGM_attack: 25.219 CW_attack: 20.917\n",
      "- Learning Progress : 32.30 %        \r"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/wangyiming/IQA/attack/deepIQA')\n",
    "from deepIQA.evaluate import eval\n",
    "# PGD TEST\n",
    "# N = Novel Approach, C = Conventional Approach\n",
    "model.eval()\n",
    "success_N = 0\n",
    "success_C = 0\n",
    "success_F = 0\n",
    "success_cw = 0\n",
    "SSIM_N_total = 0\n",
    "SSIM_C_total = 0\n",
    "SSIM_F_total = 0\n",
    "SSIM_cw_total = 0\n",
    "IQA_N_total = 0\n",
    "IQA_C_total = 0\n",
    "IQA_F_total = 0\n",
    "IQA_cw_total = 0\n",
    "total = 0\n",
    "pred_suc = 0\n",
    "attpre_suc = 0\n",
    "batch_num = 0\n",
    "for images, labels in test_loader:\n",
    "    batch_num += 1\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs_N = model(images)\n",
    "    _, ori_preds = torch.max(outputs_N, 1)   #original outputs\n",
    "    index = np.arange(labels.size(0))\n",
    "    # print(labels.size())\n",
    "    index = index[ori_preds.cpu() == labels.cpu()]\n",
    "    # print(images.size())\n",
    "    images = images[index]\n",
    "    labels = labels[index]\n",
    "    \n",
    "    #print(images.size())\n",
    "    # SSIM_attack\n",
    "    att_images_N, deltas = PGD_attack_MyLoss(model, images, labels, eps, step_size)\n",
    "    att_outputs_N = model(att_images_N)\n",
    "    _, att_preds_N = torch.max(att_outputs_N, 1)\n",
    "    success_N += (labels != att_preds_N).sum().item()\n",
    "    # success += (ori_preds != att_preds_N).sum().item()\n",
    "    # pred_suc += (ori_preds == labels).sum().item()\n",
    "    # attpre_suc += (att_preds_N == labels).sum().item()\n",
    "\n",
    "    # PGD_attack\n",
    "    att_images_C, deltas_conv = PGD_attack_Conventional(model, images, labels, eps = eps,step_size=0.025)\n",
    "    att_outputs_C = model(att_images_C)\n",
    "    _, att_preds_C = torch.max(att_outputs_C.data, 1)\n",
    "    success_C += (labels != att_preds_C).sum().item()\n",
    "\n",
    "    # FSGM_attack\n",
    "    att_images_F = fsgm_attack(model, images, labels, eps = eps)\n",
    "    att_outputs_F = model(att_images_F)\n",
    "    _, att_preds_F = torch.max(att_outputs_F.data, 1)\n",
    "    success_F += (labels != att_preds_F).sum().item()\n",
    "\n",
    "    # CW_attack\n",
    "    att_images_cw, L_inf_cw = cw_l2_attack(model, images, labels, targeted=False, c=40)\n",
    "    att_outputs_cw = model(att_images_cw)\n",
    "    _, att_preds_cw = torch.max(att_outputs_cw.data, 1)\n",
    "    success_cw += (labels != att_preds_cw).sum().item()\n",
    "\n",
    "    \n",
    "    # Display Result\n",
    "    total += labels.size(0)\n",
    "    for i in range (0, images.size()[0]):\n",
    "        SSIM_N = pytorch_ssim.ssim(images[i].unsqueeze(0),att_images_N[i].unsqueeze(0)).item()\n",
    "        SSIM_C = pytorch_ssim.ssim(images[i].unsqueeze(0),att_images_C[i].unsqueeze(0)).item()\n",
    "        SSIM_F = pytorch_ssim.ssim(images[i].unsqueeze(0),att_images_F[i].unsqueeze(0)).item()\n",
    "        SSIM_cw = pytorch_ssim.ssim(images[i].unsqueeze(0),att_images_cw[i].unsqueeze(0)).item()\n",
    "        \n",
    "        SSIM_N_total += SSIM_N\n",
    "        SSIM_C_total += SSIM_C\n",
    "        SSIM_F_total += SSIM_F\n",
    "        SSIM_cw_total += SSIM_cw\n",
    "        \n",
    "        IQA_N = eval(np.transpose(images[i].detach().cpu().numpy(),(1,2,0))*255,np.transpose(att_images_N[i].detach().cpu().numpy(),(1,2,0))*255)\n",
    "        IQA_C = eval(np.transpose(images[i].detach().cpu().numpy(),(1,2,0))*255,np.transpose(att_images_C[i].detach().cpu().numpy(),(1,2,0))*255)\n",
    "        IQA_F = eval(np.transpose(images[i].detach().cpu().numpy(),(1,2,0))*255,np.transpose(att_images_F[i].detach().cpu().numpy(),(1,2,0))*255)\n",
    "        IQA_cw = eval(np.transpose(images[i].detach().cpu().numpy(),(1,2,0))*255,np.transpose(att_images_cw[i].detach().cpu().numpy(),(1,2,0))*255)\n",
    "        \n",
    "        IQA_N_total += IQA_N\n",
    "        IQA_C_total += IQA_C\n",
    "        IQA_F_total += IQA_F\n",
    "        IQA_cw_total += IQA_cw\n",
    "        \n",
    "        \n",
    "        if(batch_num % 10 == 0):\n",
    "            l0_N = l0(images[i], att_images_N[i])\n",
    "            l0_C = l0(images[i], att_images_C[i])\n",
    "            l0_F = l0(images[i], att_images_F[i])\n",
    "            #l0_cw = l0(images[i], att_images_cw[i])\n",
    "            l1_N = l1(images[i], att_images_N[i])\n",
    "            l1_C = l1(images[i], att_images_C[i])\n",
    "            l1_F = l1(images[i], att_images_F[i])\n",
    "            #l1_cw = l1(images[i], att_images_cw[i])\n",
    "            l2_N = l2(images[i], att_images_N[i])\n",
    "            l2_C = l2(images[i], att_images_C[i])\n",
    "            l2_F = l2(images[i], att_images_F[i])\n",
    "            #l2_cw = l2(images[i], att_images_cw[i])\n",
    "            imglist = []\n",
    "            imglist.append(torchvision.utils.make_grid(images[i].data, normalize=True))\n",
    "            imglist.append(torchvision.utils.make_grid(att_images_N[i].data, normalize=True))\n",
    "            imglist.append(torchvision.utils.make_grid(att_images_C[i].data, normalize=True))\n",
    "            imglist.append(torchvision.utils.make_grid(att_images_F[i].data, normalize=True))\n",
    "            #imglist.append(torchvision.utils.make_grid(att_images_cw[i].data, normalize=True))\n",
    "            labellist = []\n",
    "            labellist.append(\"Original: \" + str(test_dataset.classes[labels[i]]))\n",
    "            labellist.append(\"SSIM_attack: \" + str(test_dataset.classes[att_preds_N[i]])\n",
    "                        + '\\n' + \"L_0 = \" + str(round(l0_N, 3)) + '\\n' + \"L_1 = \" + str(round(l1_N,3)) + \", L_2 = \" + str(round(l2_N,3))\n",
    "                        + '\\n' + 'L_inf = ' + str(round(deltas[i],3)) + ', SSIM = ' + str(round(SSIM_N,3)) + ', MOS = ' + str(round(IQA_N,3)))\n",
    "            labellist.append(\"PGD_attack: \" +str(test_dataset.classes[att_preds_C[i]])\n",
    "                        + '\\n' + \"L_0 = \" + str(round(l0_C, 3)) + '\\n' + \"L_1 = \" + str(round(l1_C,3)) + \", L_2 = \" + str(round(l2_C,3))\n",
    "                        + '\\n' + 'L_inf = ' + str(0.031) + ', SSIM = ' + str(round(SSIM_C, 3))+ ', MOS = ' + str(round(IQA_C,3)))\n",
    "            labellist.append(\"FSGM_attack: \" +str(test_dataset.classes[att_preds_F[i]])\n",
    "                        + '\\n' + \"L_0 = \" + str(round(l0_F, 3)) + '\\n' + \"L_1 = \" + str(round(l1_F,3)) + \", L_2 = \" + str(round(l2_F,3))\n",
    "                        + '\\n' + 'L_inf = ' + str(0.031) + ', SSIM = ' + str(round(SSIM_F, 3)) + ', MOS = ' + str(round(IQA_F,3)))\n",
    "            labellist.append(\"CW_attack: \" +str(test_dataset.test_labels[att_preds_cw[i]])\n",
    "                        + '\\n' + \"L_0 = \" + str(round(l0_cw, 3)) + '\\n' + \"L_1 = \" + str(round(l1_cw,3)) + \", L_2 = \" + str(round(l2_cw,3))\n",
    "                        + '\\n' + 'L_inf = ' + str(round(L_inf_cw[i],3)) + ', SSIM = ' + str(round(SSIM_cw, 3)) + ', MOS = ' + str(round(IQA_cw,3)) )\n",
    "            multi_imshow(imglist, labellist)\n",
    "    print(\"Total Image Count:\", total,\"Success Rate:\\n\", \n",
    "              \"SSIM_attack:\", success_N / total, \"PGD_attack:\", success_C / total, \"FSGM_attack:\", success_F / total,\n",
    "              \"CW_attack:\", success_cw / total)\n",
    "    print(\"Average SSIM: SSIM_attack:\",round(SSIM_N_total/total,3), \"PGD_attack:\",round(SSIM_C_total/total,3)\n",
    "         ,\"FSGM_attack:\",round(SSIM_F_total/total,3),\"CW_attack:\",round(SSIM_cw_total/total,3))\n",
    "    print(\"Average MOS: SSIM_attack:\",round(IQA_N_total/total,3), \"PGD_attack:\",round(IQA_C_total/total,3)\n",
    "         ,\"FSGM_attack:\",round(IQA_F_total/total,3),\"CW_attack:\",round(IQA_cw_total/total,3))\n",
    "    if(batch_num >=  100):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
