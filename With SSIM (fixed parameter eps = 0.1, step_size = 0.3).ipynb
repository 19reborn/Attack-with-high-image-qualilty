{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_ssim\n",
    "# https://github.com/Po-Hsun-Su/pytorch-ssim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 选择 dataset\n",
    "dataset = 'MNIST' \n",
    "# dataset = \"CIFAR-10\"\n",
    "batch_size = 50\n",
    "eps = 0.031 if dataset==\"CIFAR-10\" else 0.3\n",
    "step_size = 10 if dataset==\"CIFAR-10\" else 5\n",
    "iteration = 10\n",
    "K = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n",
    "eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_state_dict(state_dict):\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    if 'state_dict' in state_dict.keys():\n",
    "        state_dict = state_dict['state_dict']\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        if 'sub_block' in k:\n",
    "            continue\n",
    "        if 'module' in k:\n",
    "            new_state_dict[k[7:]] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    return new_state_dict\n",
    "\n",
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = torch.tensor(mean)\n",
    "        self.std = torch.tensor(std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean.type_as(x)[None, :, None, None]) / self.std.type_as(x)[None, :, None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "if(dataset == \"CIFAR-10\"):\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\"data_CIFAR10\",train=False,download=True,transform = transform)\n",
    "    test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False)\n",
    "    from model.PreActResNet18 import PreActResNet18\n",
    "    # novel CIFAR10\n",
    "    #from CIFAR10_preactresnet18 import PreActResNet18\n",
    "    #from wideresnet import WideResNet\n",
    "    #model = PreActResNet18()\n",
    "    net = PreActResNet18(num_classes=10)\n",
    "    #model = WideResNet(34, 10)\n",
    "    #check_point = torch.load(\"pretrained_model/AT-AWP_cifar10_l2_preactresnet18.pth\", map_location=torch.device(device))\n",
    "    ckpt = filter_state_dict(torch.load(\"model/AT-AWP_cifar10_l2_preactresnet18.pth\", map_location=device))\n",
    "    #check_point = torch.load(\"pretrained_model/CIFAR10_PreActResNet18.checkpoint\", map_location=torch.device(device))\n",
    "    #check_point = torch.load(\"pretrained_model/AT-AWP_cifar10_linf_wrn34-10.pth\", map_location=torch.device(device))\n",
    "    #model.load_state_dict(check_point['state_dict'])\n",
    "    #mean = (0, 0, 0)\n",
    "    #std = (1, 1, 1)\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2471, 0.2435, 0.2616)    \n",
    "    net.load_state_dict(ckpt)\n",
    "    model = nn.Sequential(Normalize(mean=mean, std=std), net)\n",
    "    model.to(device)\n",
    "else:\n",
    "    test_dataset = torchvision.datasets.MNIST(\"data_MNIST\", train=False, download=False, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    from model.small_cnn import SmallCNN\n",
    "    from model.model import LeNet5\n",
    "    from model.adv_mnist import Model\n",
    "    #model = SmallCNN()\n",
    "    model = LeNet5()\n",
    "    #model = Model()\n",
    "    #check_point = torch.load(\"pretrained_model/MNIST_small_cnn.checkpoint\", map_location=torch.device(device))\n",
    "    check_point = torch.load(\"model/adv_trained_lenet5.pkl\", map_location=torch.device(device))\n",
    "    #check_point = torch.load(\"model/adv_mnist.pth\", map_location=torch.device(device))\n",
    "    model.load_state_dict(check_point)\n",
    "    #model.load_state_dict(check_point['state_dict'])\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示一张图片（没有transpose)\n",
    "def imshow(img, title):\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(np.transpose(img.cpu(),(1,2,0)))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 并排左右显示两张图片（没有transpose)\n",
    "def double_imshow(img1,img2,title1,title2):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    i1 = plt.subplot2grid((1, 2),(0, 0))\n",
    "    plt.imshow(np.transpose(img1.cpu(),(1,2,0)))\n",
    "    i1.set_title(title1)\n",
    "    i2 = plt.subplot2grid((1, 2), (0, 1))\n",
    "    plt.imshow(np.transpose(img2.cpu(), (1, 2, 0)))\n",
    "    i2.set_title(title2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savepath = \"adv_images/SSIM, step_size = 0.15 eps = 0.3/\"\n",
    "# 并排左右显示三张图片（没有transpose)\n",
    "def triple_imshow(img1, img2, img3, title1, title2, title3, figNum = 0):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    i1 = plt.subplot2grid((1, 3),(0, 0))\n",
    "    plt.imshow(np.transpose(img1.cpu(),(1,2,0)))\n",
    "    i1.set_title(title1)\n",
    "    i2 = plt.subplot2grid((1, 3), (0, 1))\n",
    "    plt.imshow(np.transpose(img2.cpu(), (1, 2, 0)))\n",
    "    i2.set_title(title2)\n",
    "    i3 = plt.subplot2grid((1, 3), (0, 2))\n",
    "    plt.imshow(np.transpose(img3.cpu(), (1, 2, 0)))\n",
    "    i3.set_title(title3)\n",
    "    # sp = savepath + \"img\" + str(figNum) + '.png'\n",
    "    # plt.savefig(sp)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l2 Loss\n",
    "def l2(img1,img2):\n",
    "    channel = img1.size()[0]\n",
    "    height = img1.size()[1]\n",
    "    width = img1.size()[2]\n",
    "    total_dif = 0.0\n",
    "    for i in range(0,channel):\n",
    "        for j in range(0,height):\n",
    "            for k in range(0, width):\n",
    "                total_dif += (img1[i][j][k].item()-img2[i][j][k].item()) ** 2\n",
    "    return total_dif ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "img = cv.imread(\"img.png\")\n",
    "cv.imshow(\"Image\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1 Loss\n",
    "def l1(img1,img2):\n",
    "    channel = img1.size()[0]\n",
    "    height = img1.size()[1]\n",
    "    width = img1.size()[2]\n",
    "    total_dif = 0\n",
    "    for i in range(0, channel):\n",
    "        for j in range(0, height):\n",
    "            for k in range(0, width):\n",
    "                total_dif += abs(img1[i][j][k].item() - img2[i][j][k].item())\n",
    "    return total_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l0 Loss 计算差距<=0.01像素点的数目\n",
    "def l0(img1,img2):\n",
    "    channel = img1.size()[0]\n",
    "    height = img1.size()[1]\n",
    "    width = img1.size()[2]\n",
    "    total_dif = 0\n",
    "    for i in range(0, channel):\n",
    "        for j in range(0, height):\n",
    "            for k in range(0, width):\n",
    "                total_dif += 0 if abs(img1[i][j][k].item() - img2[i][j][k].item()) <= 0.01 else 1\n",
    "    return total_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, delta, y, images, lam):\n",
    "        mask = torch.ones_like(x).scatter_(1, y.unsqueeze(1), 0.)\n",
    "        rest = x[mask.bool()].view(x.size(0), x.size(1) - 1)\n",
    "        xx = torch.tensor(range(x.size(0)))\n",
    "        f = torch.nn.ReLU()\n",
    "        return torch.mean(f(x[xx, y] - torch.max(rest, 1)[0] + K) + lam *(1 - (pytorch_ssim.ssim(images,images+delta))))\n",
    "        #return torch.mean(f(x[xx, y] - torch.max(rest, 1)[0] + K) + lam *(eps + torch.max(delta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_attack_MyLoss(net, images, labels, eps, step_size):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    delta = torch.zeros(images.size()).to(device)  #变量求导\n",
    "    lam = torch.zeros(1).to(device)\n",
    "    #delta.uniform_(-eps, eps).to(device)\n",
    "    delta = Variable(delta, requires_grad=True)\n",
    "    lam = Variable(lam,requires_grad=True)\n",
    "    # print(images.size())\n",
    "    # print(labels.size())\n",
    "    # ori_images = images.data\n",
    "    criterion = My_loss()\n",
    "    for i in range(iteration):\n",
    "        # delta.requires_grad = True\n",
    "        # lam.requires_grad = True\n",
    "        # print(images.size())\n",
    "        outputs = net(images + delta)\n",
    "        net.zero_grad()\n",
    "        loss = criterion(outputs, delta, labels,images,lam).to(device)\n",
    "        loss.backward()\n",
    "\n",
    "        # print(delta.grad)\n",
    "        new_delta = delta - step_size * delta.grad.detach()\n",
    "        adv_images = torch.clamp(images + new_delta, min=0, max=1)\n",
    "        delta = adv_images - images\n",
    "        new_lam = lam + step_size * lam.grad.detach()\n",
    "        if new_lam < 0:\n",
    "            lam = torch.zeros(1).to(device)\n",
    "        else:\n",
    "            lam = new_lam\n",
    "        # delta = torch.clamp(delta, min=-eps, max=eps)\n",
    "        delta = Variable(delta, requires_grad=True)\n",
    "        lam = Variable(lam,requires_grad=True)\n",
    "        # for i in range(0, images.size()[0]):\n",
    "        #     print(torch.max(delta[i]).item())\n",
    "        #     print(torch.min(delta[i]).item())\n",
    "    deltas = []\n",
    "    for i in range(0,images.size()[0]):\n",
    "        deltas.append(torch.max(torch.abs(delta[i])).item())\n",
    "    return images + delta, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PGD_attack_Conventional(model, image, label, eps, step_size, iters=10):\n",
    "    image = image.to(device)\n",
    "    label = label.to(device)\n",
    "    loss = nn.NLLLoss()\n",
    "    ori_image = image.data\n",
    "\n",
    "    for i in range(iters):\n",
    "        image.requires_grad = True\n",
    "        output = model(image)\n",
    "        model.zero_grad()\n",
    "        cost = loss(output, label).to(device)\n",
    "        cost.backward()\n",
    "        adv_image = image + step_size * image.grad.sign()\n",
    "        delta = torch.clamp(adv_image - ori_image, min=-eps, max=eps)\n",
    "        image = torch.clamp(ori_image + delta, min=0, max=1).detach_()\n",
    "    deltas = []\n",
    "    delta = image-ori_image \n",
    "    for i in range(0,images.size()[0]):\n",
    "        deltas.append(torch.max(torch.abs(delta[i])).item())\n",
    "    return image ,deltas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Image Count: 5 Success Rate:\n",
      " Novel Approach: 0.9387755102040817 Conventional Approach: 0.8979591836734694\n",
      "Total Image Count: 10 Success Rate:\n",
      " Novel Approach: 0.9175257731958762 Conventional Approach: 0.8969072164948454\n",
      "Total Image Count: 15 Success Rate:\n",
      " Novel Approach: 0.9310344827586207 Conventional Approach: 0.903448275862069\n",
      "Total Image Count: 20 Success Rate:\n",
      " Novel Approach: 0.9476439790575916 Conventional Approach: 0.9162303664921466\n",
      "Total Image Count: 25 Success Rate:\n",
      " Novel Approach: 0.9458333333333333 Conventional Approach: 0.9166666666666666\n",
      "Total Image Count: 30 Success Rate:\n",
      " Novel Approach: 0.9508771929824561 Conventional Approach: 0.9192982456140351\n",
      "Total Image Count: 35 Success Rate:\n",
      " Novel Approach: 0.9550898203592815 Conventional Approach: 0.9251497005988024\n",
      "Total Image Count: 40 Success Rate:\n",
      " Novel Approach: 0.9528795811518325 Conventional Approach: 0.9240837696335078\n",
      "Total Image Count: 45 Success Rate:\n",
      " Novel Approach: 0.9489559164733179 Conventional Approach: 0.9234338747099768\n"
     ]
    }
   ],
   "source": [
    "# PGD TEST\n",
    "# N = Novel Approach, C = Conventional Approach\n",
    "model.eval()\n",
    "success_N = 0\n",
    "success_C = 0\n",
    "total = 0\n",
    "pred_suc = 0\n",
    "attpre_suc = 0\n",
    "batch_num = 0\n",
    "for images, labels in test_loader:\n",
    "    batch_num += 1\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    outputs_N = model(images)\n",
    "    _, ori_preds = torch.max(outputs_N, 1)   #original outputs\n",
    "    index = np.arange(labels.size(0))\n",
    "    # print(labels.size())\n",
    "    index = index[ori_preds.cpu() == labels.cpu()]\n",
    "    # print(images.size())\n",
    "    images = images[index]\n",
    "    labels = labels[index]\n",
    "\n",
    "    # Novel Approach\n",
    "    att_images_N, deltas = PGD_attack_MyLoss(model, images, labels, eps, step_size)\n",
    "    att_outputs_N = model(att_images_N)\n",
    "    _, att_preds_N = torch.max(att_outputs_N, 1)\n",
    "    success_N += (labels != att_preds_N).sum().item()\n",
    "    # success += (ori_preds != att_preds_N).sum().item()\n",
    "    # pred_suc += (ori_preds == labels).sum().item()\n",
    "    # attpre_suc += (att_preds_N == labels).sum().item()\n",
    "\n",
    "    # Conventional Approach\n",
    "    att_images_C, deltas_conv = PGD_attack_Conventional(model, images, labels, eps,step_size=0.025)\n",
    "    att_outputs_C = model(att_images_C)\n",
    "    _, att_preds_C = torch.max(att_outputs_C.data, 1)\n",
    "    success_C += (labels != att_preds_C).sum().item()\n",
    "\n",
    "    # Display Result\n",
    "    total += labels.size(0)\n",
    "    if(batch_num % 10 == 0):\n",
    "        for i in range (0, images.size()[0]):\n",
    "            triple_imshow(torchvision.utils.make_grid(images[i].data, normalize=True),\n",
    "                        torchvision.utils.make_grid(att_images_N[i].data, normalize=True),\n",
    "                        torchvision.utils.make_grid(att_images_C[i].data, normalize=True),\n",
    "                        \"Original: \" + str(test_dataset.test_labels[labels[i]]),\n",
    "                        \"Novel Approach: \" + str(test_dataset.test_labels[att_preds_N[i]])\n",
    "                        + '\\n' + \"L_0 = \" + str(round(l0(images[i], att_images_N[i]), 3))\n",
    "                        + '\\n' + \"L_1 = \" + str(round(l1(images[i],att_images_N[i]),3))\n",
    "                        + \", L_2 = \" + str(round(l2(images[i], att_images_N[i]),3))\n",
    "                        + '\\n' + 'L_inf = ' + str(round(deltas[i],3)) + ', SSIM = ' +\n",
    "                        str(round(pytorch_ssim.ssim(images[i].unsqueeze(0),att_images_N[i].unsqueeze(0)).item(),3)),\n",
    "                        \"Conventional Approach: \" +str(test_dataset.test_labels[att_preds_C[i]])\n",
    "                        + '\\n' + \"L_0 = \" + str(round(l0(images[i], att_images_C[i]), 3))\n",
    "                        + '\\n' + \"L_1 = \" + str(round(l1(images[i],att_images_C[i]),3))\n",
    "                        + \", L_2 = \" + str(round(l2(images[i],att_images_C[i]),3))\n",
    "                        + '\\n' + 'L_inf = ' + str(round(deltas_conv[i],3)) + ', SSIM = ' +\n",
    "                        str(round(pytorch_ssim.ssim(images[i].unsqueeze(0), att_images_C[i].unsqueeze(0)).item(), 3)),\n",
    "                        5 * batch_num - 4 + i)\n",
    "    #     double_imshow(torchvision.utils.make_grid(images[i].data, normalize=True),\n",
    "    #                     torchvision.utils.make_grid(att_images_N[i].data, normalize=True),\n",
    "    #                     \"Original: \" + str(train_dataset.classes[labels[i]]),\n",
    "    #                     \"Novel Approach: \" + str(train_dataset.classes[att_preds_N[i]])\n",
    "    #                     + '\\n' + str(round(deltas[i],3)))\n",
    "    #     print('SSIM_N = ' + str(round(pytorch_ssim.ssim(images[i].unsqueeze(0),att_images_N[i].unsqueeze(0)).item(),3)))\n",
    "    #     print('SSIM_C = ' + str(round(pytorch_ssim.ssim(images[i].unsqueeze(0), att_images_C[i].unsqueeze(0)).item(), 3)))\n",
    "\n",
    "    print(\"Total Image Count:\", 5 * batch_num,\"Success Rate:\\n\", \"Novel Approach:\", success_N / total, \"Conventional Approach:\", success_C / total)\n",
    "    if(batch_num >=  300):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
